{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "This Jupyter Notebook is dedicated to understanding and implementing the gradient descent algorithm on soccer data. You can find the dataset [2022-2023 Soccer Player Stats Dataset](https://www.kaggle.com/datasets/vivovinco/20222023-football-player-stats?resource=download).\n",
    "\n",
    "The following packages are required to run the attached code:\n",
    "\n",
    "- [Plotly](https://plotly.com/python/)\n",
    "\n",
    "- [Plotly Express](https://plotly.com/python/plotly-express/)\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/docs/)\n",
    "\n",
    "- [Matplotlib.pylab](https://matplotlib.org/2.0.2/api/pyplot_api.html)\n",
    "\n",
    "- [Numpy](https://numpy.org/doc/)\n",
    "\n",
    "- [Seaborn](https://seaborn.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Gradient descent is a generic method for optimization that searches for an optimial solution by iteratively moving along a function in search of better solutions. \n",
    "\n",
    "Mathematically, gradient descent attempts minimize f(x) for x âˆˆ â„, f(x) differentiable.\n",
    "\n",
    "With perceptron, we saw that we can train a single neurons by iteratively updating our weights and bias. Gradient descent is one simple yet effective way to train a neural network.\n",
    "\n",
    "Notably, this algorithm will allow us to find a local minimum, but is not guaranteed to find the global minimum (these can be far different!).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "1. Get a starting point: To begin, we just need a first guess. Ideally this would be \"close\" to the global minimum, but sometimes we have no idea where that might be. Any x where f(x) exists will work. \n",
    "\n",
    "2. Compute the gradient: The gradient, or derivative for first order functions, will tell us the direction of the nearest local minimum. If the tangent line at a point has a positive slope, it means the function is increasing, so we'll want to go to the left to get to a \"lower point\". If the tangent line at a point is negative, it means it's decreasing as we move to the right, so we will go to the right.\n",
    "\n",
    "3. Select a step size / learning rate: This is the distance we want to move (after being multiplied by the gradient) at each iteration. A larger step size will have a more drastic move while smaller ones will take a more precise step towards the local minimum. Different functions will have different \"best\" step sizes, we can make predictions based on the functions, but oftentimes, experimenting with the data will get you to a good result.\n",
    "\n",
    "4. Select a stop condition: Over infinitely many iterations, gradient descent will get you exactly to the local minimum, but as the gradient gets closer and closer to 0, we may want to introduce a stopping condition so our code doesn't run forever. This can look like a number of iterations to do descent or a threshold for the gradient or the change in the cost function at each iteration. \n",
    "\n",
    "5. Repeat 2-4 taking the initial starting point to now be the previous point - the gradient * step size.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Import the necessary modules.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on a Single Variable Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Perform gradient descent on x^4 -3x^3 + 2x^2. Note that this function has local minima at x = 0 and x = 1.64.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6405275076253054"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**4  - 3 * x**3 + 2 * x**2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 4 * x**3 - 9 * x**2 + 4 * x\n",
    "\n",
    "def g_descent(f_prime, alpha = 0.8, w_0 = 5.0, max_iter = 1000):\n",
    "    W = [w_0]\n",
    "    i = 0\n",
    "    while abs(f_prime(W[-1])) > 0.001 and i < max_iter:\n",
    "        w_new = W[-1] - alpha*f_prime(W[-1])\n",
    "        W.append(w_new)\n",
    "        i += 1\n",
    "    W = np.array(W)\n",
    "    return W[-1]\n",
    "\n",
    "l_min = g_descent(f_prime, 0.01)\n",
    "l_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note that starting with different initial values can, in fact, change the local minimum found.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local minimum at x = -0.0 for x_0 = -2\n",
      "Local minimum at x = 1.64 for x_0 = 1\n",
      "Local minimum at x = 1.64 for x_0 = 3\n"
     ]
    }
   ],
   "source": [
    "# Set x values.\n",
    "x1 = -2\n",
    "x2 = 1\n",
    "x3 = 3\n",
    "\n",
    "# Perform gradient descent at the different initial values. \n",
    "g1 = g_descent(f_prime, 0.01, x1)\n",
    "g2 = g_descent(f_prime, 0.01, x2)\n",
    "g3 = g_descent(f_prime, 0.01, x3)\n",
    "\n",
    "# Print the resulting minima.\n",
    "print(f'Local minimum at x = {round(g1,2)} for x_0 = {x1}')\n",
    "print(f'Local minimum at x = {round(g2,2)} for x_0 = {x2}')\n",
    "print(f'Local minimum at x = {round(g3,2)} for x_0 = {x3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on a More Complex Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
